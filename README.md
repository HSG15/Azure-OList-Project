## ðŸš€ Azure Data Engineering Project - Olist E-commerce Data Pipeline

A cloud-based data engineering project that demonstrates real-world data ingestion, processing, and transformation using Azure services following the **Medallion Architecture (Bronze â†’ Silver â†’ Gold)**.

---

### ðŸ“Œ Project Architecture

![Architecture Diagram](Architecture%20Diagram.png)

---

### ðŸ§° Tools & Services Used

- **Azure Data Factory (ADF)** â€“ For orchestrating data ingestion from multiple sources  
- **Azure Data Lake Storage Gen2 (ADLS Gen2)** â€“ For storing raw and processed data  
- **Azure Databricks** â€“ For data transformation using PySpark  
- **MongoDB ([filess.io](https://dash.filess.io/))** â€“ As a NoSQL enrichment source  
- **MySQL ([filess.io](https://dash.filess.io/))** â€“ As a relational source for some datasets  
- **GitHub (via HTTP)** â€“ For hosting static datasets  
- **Azure Synapse Analytics** â€“ For querying transformed data and pushing to Gold layer  
- **Power BI / Tableau / Microsoft Fabric** â€“ For data visualization (optional)

---

### ðŸ§­ Project Flow (Step-by-Step)

1. **ðŸ” Azure Setup**
   - Created Azure account and resource group: `ecommerce-live`
   - Provisioned necessary services: ADF, ADLS Gen2, Databricks, Synapse

2. **ðŸ—ï¸ Data Lake Setup (Medallion Architecture)**
   - Created ADLS Gen2 Storage: `hsgolistecommstorage`
   - Added container: `olist-data` with directories: `/bronze`, `/silver`, `/gold`

3. **ðŸ”„ Data Ingestion using Azure Data Factory**
   - Created ADF pipeline to:
     - Copy data from GitHub (via HTTP) to `/bronze`
     - Ingest data from MySQL using linked service
     - Used JSON config with `csv_relative_url` and `file_name` inside `ForEach` loop
     - Triggered dependent copy activity to move SQL data to `/bronze`

4. **ðŸ”— Configuration for Azure Databricks Access**
   - Registered App in Azure Active Directory  
   - Created Client Secret and assigned **Storage Blob Data Contributor** role  
   - Connected Databricks to ADLS Gen2

5. **ðŸ§ª Data Transformation using Azure Databricks**
   - Loaded raw data from ADLS Gen2 `/bronze`
   - Retrieved additional data from MongoDB hosted on [filess.io](https://dash.filess.io/)
   - Merged and cleaned datasets using PySpark
   - Wrote processed data to ADLS Gen2 `/silver` in **Parquet** format

6. **ðŸ” Data Serving using Azure Synapse**
   - Created Synapse workspace and storage account: `hsg-synapse-storage`
   - Set up Serverless SQL pool and connected to ADLS `/silver`
   - Queried and filtered transformed data (e.g., `order_status = 'delivered'`)
   - Created a view and exported final dataset to `/gold` layer using CETAS  
     ðŸ‘‰ [Learn about CETAS in Synapse](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-cetas)

7. **ðŸ“Š Data Visualization (Optional)**
   - Final datasets from `/gold` are visualization-ready
   - Tools: Power BI / Tableau / Microsoft Fabric

---

### âš™ï¸ Setup Instructions (Local + Azure)

> **Prerequisites:**  
> - Azure Subscription  
> - Databricks Workspace  
> - GitHub account  
> - MongoDB (hosted or cloud)  
> - MySQL (hosted or cloud)

1. **Clone the Repo**
   ```bash
   git clone https://github.com/yourusername/azure-olist-project.git
   cd azure-olist-project
   ```

2. **Upload Datasets**
   - Push static CSVs to GitHub repo (for HTTP access)
   - Upload one dataset to MySQL DB ([filess.io](https://dash.filess.io/))
   - Push another dataset to MongoDB ([filess.io](https://dash.filess.io/))

3. **Configure Azure Resources**
   - ADF pipelines
   - Linked services (GitHub, MySQL, ADLS Gen2)
   - App Registration & Role Assignment for Databricks

4. **Run Databricks Notebooks**
   - Load data from `/bronze` and MongoDB
   - Clean, transform, join
   - Write output to `/silver` as Parquet

5. **Connect Synapse**
   - Access `/silver` in serverless SQL pool
   - Create views and use CETAS to export to `/gold`  
     (Reference: [CETAS Docs](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-cetas))

6. **Visualize (Optional)**
   - Use Power BI or Tableau to connect to `/gold`

---

### ðŸ“‚ Folder Structure

```
.
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ databricks_etl.py
â”œâ”€â”€ adf/
â”‚   â””â”€â”€ pipeline.json
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_input.csv
â”œâ”€â”€ Architecture Diagram.png
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

### ðŸ“ˆ Key Concepts

- Medallion Architecture: Bronze (raw) â†’ Silver (cleaned) â†’ Gold (analytics-ready)
- PySpark transformations in Databricks
- CETAS (Create External Table As Select) in Synapse
- JSON-driven dynamic ingestion pipeline in ADF

---

### ðŸ™Œ Acknowledgements

- [Azure Docs](https://learn.microsoft.com/en-us/azure/)
- [filess.io](https://dash.filess.io/) for free DB hosting  
- [Kaggle](https://www.kaggle.com/) for datasets  
- Community blogs and tutorials on data lakehouse and medallion design  
